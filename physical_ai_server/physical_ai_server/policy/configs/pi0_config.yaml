# Pi0 Policy Configuration
# Based on LeRobot's configuration_pi0.py

policy_name: "pi0"

# PaliGemma Vision-Language Model Configuration
paligemma:
  pretrained_model_name_or_path: "google/paligemma-3b-mix-224"
  revision: "main"
  torch_dtype: "float16"  # Options: float16, float32, bfloat16
  
  # Model architecture settings
  vision_encoder:
    image_size: 224
    patch_size: 14
    num_channels: 3
    hidden_size: 1024
    num_hidden_layers: 24
    num_attention_heads: 16
    intermediate_size: 4096
    
  # Language model settings
  text_config:
    vocab_size: 257152
    hidden_size: 2048
    intermediate_size: 16384
    num_hidden_layers: 18
    num_attention_heads: 8
    num_key_value_heads: 1
    head_dim: 256
    hidden_act: "gelu_pytorch_tanh"
    max_position_embeddings: 4096
    
  # Projection layer settings
  projection_dim: 2048
  hidden_size: 2048

# Tokenizer Configuration
tokenizer:
  model_name_or_path: "google/paligemma-3b-mix-224"
  padding_side: "right"
  truncation: true
  max_length: 512
  add_special_tokens: true
  
  # Special tokens
  bos_token: "<bos>"
  eos_token: "<eos>"
  unk_token: "<unk>"
  pad_token: "<pad>"

# Projector Configuration for Action Output
projector:
  # Input from language model hidden states
  input_dim: 2048  # Must match PaliGemma hidden_size
  
  # Action space configuration  
  action_dim: null  # Will be set based on environment
  
  # Projector architecture
  hidden_dims: [1024, 512]  # Hidden layer dimensions
  activation: "relu"  # Options: relu, gelu, silu, tanh
  dropout: 0.1
  layer_norm: true
  
  # Output configuration
  output_activation: null  # No activation for continuous actions
  use_bias: true

# Flow Matching Configuration (Alternative to Diffusion)
flow_matching:
  enabled: true  # Whether to use flow matching for action generation
  
  # Flow parameters
  sigma_min: 1e-4
  time_steps: 50
  
  # Conditional flow matching
  conditional: true
  
  # Training configuration
  loss_type: "flow_matching"  # Options: flow_matching, mse
  
# Sequence Processing Configuration
sequence:
  # Context length for observations
  context_length: 10  # Number of past observations to condition on
  
  # Action prediction horizon
  action_horizon: 1  # Number of future actions to predict
  
  # Token processing
  max_tokens: 512
  token_embedding_dim: 2048

# Fine-tuning Configuration
fine_tuning:
  # LoRA (Low-Rank Adaptation) settings
  lora:
    enabled: true
    rank: 16
    alpha: 32
    dropout: 0.1
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
    
  # Freezing strategy
  freeze_vision_encoder: true
  freeze_language_model: false  # Allow fine-tuning of language model
  freeze_projector: false
  
  # Gradient checkpointing
  gradient_checkpointing: true

# Training Configuration
training:
  # Optimizer settings
  optimizer:
    name: "AdamW"
    lr: 5.0e-5
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1.0e-8
    
  # Learning rate scheduler
  lr_scheduler:
    name: "cosine_with_restarts"
    warmup_steps: 1000
    num_cycles: 1
    
  # Training hyperparameters
  batch_size: 16
  accumulate_grad_batches: 4  # Effective batch size = 64
  num_epochs: 100
  gradient_clip_val: 1.0
  
  # Mixed precision training
  precision: "16-mixed"  # Options: 32, 16-mixed, bf16-mixed
  
  # Validation settings
  val_check_interval: 0.25  # Check every 25% of epoch
  save_checkpoint: true
  
  # Loss configuration
  loss:
    name: "mse"  # Options: mse, l1, huber
    reduction: "mean"
    label_smoothing: 0.0

# Data Processing Configuration
data:
  # Image preprocessing
  image_preprocessing:
    resize: [224, 224]  # Resize to PaliGemma input size
    normalize: true
    mean: [0.485, 0.456, 0.406]  # ImageNet stats
    std: [0.229, 0.224, 0.225]
    
  # Text prompt configuration
  text_prompts:
    instruction_template: "What action should the robot take? {instruction}"
    default_instruction: "Perform the task"
    max_instruction_length: 100
    
  # Data keys mapping
  image_keys: ["observation.images.cam_high", "observation.images.cam_low", "observation.images.cam_left_wrist", "observation.images.cam_right_wrist"]
  action_keys: ["action"]
  state_keys: ["observation.state"]
  
  # Temporal sampling
  delta_timestamps:
    action: [0.0]  # Current action
    observation.images.cam_high: [0.0]
    observation.images.cam_low: [0.0]
    observation.images.cam_left_wrist: [0.0]
    observation.images.cam_right_wrist: [0.0]
    observation.state: [0.0]

# Evaluation Configuration
evaluation:
  n_episodes: 50
  max_episode_steps: 400
  
  # Metrics to track
  metrics:
    - "success_rate"
    - "episode_length"
    - "reward"
    - "action_mse"

# Hardware Configuration
device:
  type: "auto"  # Options: auto, cpu, cuda, mps
  compile: false  # Whether to use torch.compile
  
  # Memory optimization
  gradient_checkpointing: true
  use_cpu_offload: false

# Model Checkpointing
checkpoint:
  save_dir: "./checkpoints/pi0"
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"
  
  # Checkpoint strategy
  save_strategy: "epoch"  # Options: epoch, steps
  save_steps: 1000

# Logging Configuration
logging:
  wandb:
    enabled: false
    project: "pi0_policy"
    entity: null
    
  tensorboard:
    enabled: true
    log_dir: "./logs/pi0"
    
  # Log frequency
  log_every_n_steps: 50

# Environment-specific presets
presets:
  aloha:
    paligemma:
      torch_dtype: "float16"
    projector:
      hidden_dims: [1024, 512]
    sequence:
      context_length: 10
      action_horizon: 1
    data:
      image_preprocessing:
        resize: [224, 224]
    
  pusht:
    paligemma:
      torch_dtype: "float16"
    projector:
      hidden_dims: [512, 256]
    sequence:
      context_length: 5
      action_horizon: 1
    data:
      image_preprocessing:
        resize: [224, 224]
        
  manipulation:
    paligemma:
      torch_dtype: "float16"
    projector:
      hidden_dims: [1024, 512, 256]
    sequence:
      context_length: 15
      action_horizon: 1
    fine_tuning:
      lora:
        rank: 32
        alpha: 64
    data:
      text_prompts:
        instruction_template: "Given the current scene, what should the robot do to {instruction}?"

# Model Loading Configuration
model_loading:
  pretrained_policy_name_or_path: null  # Path to pretrained Pi0 model
  revision: "main"
  local_files_only: false
  trust_remote_code: false
  
  # Model format
  save_format: "safetensors"  # Options: safetensors, pytorch
