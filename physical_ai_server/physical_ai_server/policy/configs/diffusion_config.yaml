# Diffusion Policy Configuration
# Based on LeRobot's configuration_diffusion.py

policy_name: "diffusion"

# Vision Backbone Configuration
vision_backbone:
  name: "resnet18"  # Options: resnet18, resnet34, convnet  
  pretrained: true
  replace_final_stride_with_dilation: false
  
  # Vision encoder configurations for different backbones
  resnet18:
    n_channels: 3
    pretrained: true
    replace_final_stride_with_dilation: false
  
  resnet34: 
    n_channels: 3
    pretrained: true
    replace_final_stride_with_dilation: false
  
  convnet:
    n_channels: 3
    down_dims: [512, 1024, 2048]

# Spatial Softmax Configuration
spatial_softmax:
  enabled: false
  num_kp: null
  learnable_temperature: false
  temperature: 1.0
  noise_std: 0.0

# U-Net Architecture Configuration
unet:
  sample_size: 16
  in_channels: 16  # action_dim + observation_features_dim
  down_block_types:
    - "DownBlock1D"
    - "DownBlock1D" 
    - "DownBlock1D"
    - "AttnDownBlock1D"
  up_block_types:
    - "AttnUpBlock1D"
    - "UpBlock1D"
    - "UpBlock1D"
    - "UpBlock1D"
  block_out_channels: [32, 128, 256, 512]
  layers_per_block: 1
  norm_num_groups: 8
  use_timestep_embedding: true
  attention_head_dim: 64
  
  # Model configurations
  diffusion_step_embed_dim: 128
  down_dims: [512, 1024, 2048]

# Noise Scheduler Configuration  
noise_scheduler:
  name: "DDPM"  # Options: DDPM, DDIM
  
  # DDPM Configuration
  ddpm:
    num_train_timesteps: 100
    beta_start: 0.0001
    beta_end: 0.02
    beta_schedule: "squaredcos_cap_v2"  # Options: linear, scaled_linear, squaredcos_cap_v2
    variance_type: "fixed_small"  # Options: fixed_small, fixed_large, learned, learned_range
    clip_sample: true
    prediction_type: "epsilon"  # Options: epsilon, sample, v_prediction
    
  # DDIM Configuration
  ddim:
    num_train_timesteps: 100
    beta_start: 0.0001
    beta_end: 0.02
    beta_schedule: "squaredcos_cap_v2"
    clip_sample: true
    set_alpha_to_one: true
    steps_offset: 1
    prediction_type: "epsilon"

# Diffusion Policy Specific Parameters
diffusion:
  horizon: 16  # Number of future steps to predict
  n_action_steps: 8  # Number of action steps to execute
  n_obs_steps: 2  # Number of observation steps to condition on
  num_inference_steps: 10  # Number of denoising steps during inference
  
  # Action and observation dimensions (to be set by policy)
  action_dim: null  # Will be set based on environment
  observation_features_dim: null  # Will be set based on vision backbone

# Crop Configuration for Camera Images
crop:
  enabled: false
  top: 0
  left: 0
  height: 480
  width: 640

# Training Configuration
training:
  # Optimizer settings
  optimizer:
    name: "AdamW"
    lr: 1.0e-4
    weight_decay: 1.0e-6
    betas: [0.95, 0.999]
    eps: 1.0e-8
  
  # Learning rate scheduler
  lr_scheduler:
    name: "cosine"
    warmup_steps: 500
    
  # Training hyperparameters
  batch_size: 64
  num_epochs: 1000
  gradient_clip_val: 10.0
  
  # Validation settings
  val_check_interval: 250
  save_checkpoint: true
  
  # Loss configuration
  loss:
    name: "mse"  # Options: mse, l1, huber
    reduction: "mean"

# Evaluation Configuration
evaluation:
  n_episodes: 50
  max_episode_steps: 400

# Hardware Configuration
device:
  type: "auto"  # Options: auto, cpu, cuda, mps
  compile: false  # Whether to use torch.compile

# Model Checkpointing
checkpoint:
  save_dir: "./checkpoints/diffusion"
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"

# Logging Configuration
logging:
  wandb:
    enabled: false
    project: "diffusion_policy"
    entity: null
  
  tensorboard:
    enabled: true
    log_dir: "./logs/diffusion"

# Data Configuration
data:
  image_keys: ["observation.images.cam_high", "observation.images.cam_low", "observation.images.cam_left_wrist", "observation.images.cam_right_wrist"]
  delta_timestamps:
    action: [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4]
    observation.images.cam_high: [-0.2, 0.0]
    observation.images.cam_low: [-0.2, 0.0] 
    observation.images.cam_left_wrist: [-0.2, 0.0]
    observation.images.cam_right_wrist: [-0.2, 0.0]
    observation.state: [-0.2, 0.0]

# Environment-specific presets
presets:
  aloha:
    vision_backbone:
      name: "resnet18"
    crop:
      enabled: true
      top: 0
      left: 0  
      height: 480
      width: 640
    diffusion:
      horizon: 16
      n_action_steps: 8
      n_obs_steps: 2
      
  pusht:
    vision_backbone:
      name: "resnet18"
    crop:
      enabled: false
    diffusion:
      horizon: 16
      n_action_steps: 8
      n_obs_steps: 2
      
  xarm:
    vision_backbone:
      name: "resnet18"
    crop:
      enabled: true
      top: 0
      left: 0
      height: 480  
      width: 640
    diffusion:
      horizon: 16
      n_action_steps: 8
      n_obs_steps: 2
