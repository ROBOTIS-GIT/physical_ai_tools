# GR00T N1 Policy Configuration
# Based on Isaac-GR00T/gr00t/model/gr00t_n1.py

policy_name: "gr00t_n1"

# Backbone Configuration (Eagle Model)
backbone:
  name: "eagle"
  
  # Model architecture
  architecture:
    hidden_size: 4096
    num_hidden_layers: 32
    num_attention_heads: 32
    num_key_value_heads: 8
    intermediate_size: 14336
    vocab_size: 128256
    max_position_embeddings: 8192
    
  # Attention configuration
  attention:
    attention_dropout: 0.0
    hidden_dropout: 0.0
    rope_theta: 500000.0
    rope_scaling: null
    use_sliding_window: false
    sliding_window: null
    
  # Model loading
  pretrained_model_name_or_path: "AIDC-AI/Marco-o1"
  revision: "main"
  torch_dtype: "bfloat16"  # Options: float16, float32, bfloat16
  
  # Memory optimization
  use_cache: true
  gradient_checkpointing: false

# Action Head Configuration (Diffusion Transformer)
action_head:
  name: "diffusion_transformer"
  
  # Architecture settings
  architecture:
    input_dim: 4096  # Must match backbone hidden_size
    action_dim: null  # Will be set based on environment
    hidden_dim: 2048
    num_layers: 8
    num_heads: 16
    dropout: 0.1
    
  # Diffusion parameters
  diffusion:
    num_timesteps: 100
    beta_schedule: "linear"  # Options: linear, cosine, sigmoid
    beta_start: 0.0001
    beta_end: 0.02
    
    # Sampling configuration
    num_inference_steps: 20
    guidance_scale: 1.0
    eta: 0.0  # DDIM parameter
    
  # Conditioning
  conditioning:
    type: "cross_attention"  # Options: cross_attention, concat, add
    condition_dim: 4096
    
# Sequence Processing Configuration
sequence:
  # Input sequence configuration
  max_sequence_length: 512
  context_window: 100  # Number of past observations to consider
  
  # Action sequence configuration
  action_horizon: 16  # Number of future actions to predict
  action_chunking: 8   # Number of actions to execute per step
  
  # Padding and truncation
  padding: "max_length"
  truncation: true

# Embodiment Configuration
embodiment:
  # Robot embodiment tags
  robot_type: "humanoid"  # Options: humanoid, quadruped, manipulator
  
  # Embodiment-specific settings
  humanoid:
    num_joints: 23
    joint_names: [
      "torso_lift_joint", "head_pan_joint", "head_tilt_joint",
      "shoulder_pan_left_joint", "shoulder_lift_left_joint", "upperarm_roll_left_joint",
      "elbow_flex_left_joint", "forearm_roll_left_joint", "wrist_flex_left_joint", "wrist_roll_left_joint",
      "shoulder_pan_right_joint", "shoulder_lift_right_joint", "upperarm_roll_right_joint",
      "elbow_flex_right_joint", "forearm_roll_right_joint", "wrist_flex_right_joint", "wrist_roll_right_joint",
      "hip_pitch_left_joint", "knee_joint_left", "ankle_pitch_left_joint",
      "hip_pitch_right_joint", "knee_joint_right", "ankle_pitch_right_joint"
    ]
    
    # Action space bounds
    action_bounds:
      position: [-3.14159, 3.14159]  # Joint position limits
      velocity: [-10.0, 10.0]       # Joint velocity limits
      
  manipulator:
    num_joints: 7
    joint_names: ["joint1", "joint2", "joint3", "joint4", "joint5", "joint6", "joint7"]
    action_bounds:
      position: [-3.14159, 3.14159]
      velocity: [-5.0, 5.0]

# Vision Processing Configuration
vision:
  # Image encoder settings
  image_encoder:
    name: "clip"  # Options: clip, resnet, efficientnet
    model_name: "openai/clip-vit-large-patch14"
    
    # Image preprocessing
    image_size: 224
    patch_size: 14
    num_channels: 3
    
    # Feature extraction
    feature_dim: 1024
    pool_type: "avg"  # Options: avg, max, cls
    
  # Multi-view processing
  multi_view:
    enabled: true
    num_views: 4  # front, left, right, wrist cameras
    fusion_method: "attention"  # Options: attention, concat, average
    
  # Spatial attention
  spatial_attention:
    enabled: true
    num_patches: 196  # 14x14 patches for 224x224 image
    attention_heads: 8

# Training Configuration
training:
  # Optimizer settings
  optimizer:
    name: "AdamW"
    lr: 1.0e-4
    weight_decay: 0.01
    betas: [0.9, 0.95]
    eps: 1.0e-8
    
  # Learning rate scheduler
  lr_scheduler:
    name: "cosine"
    warmup_steps: 2000
    max_steps: 100000
    
  # Training hyperparameters
  batch_size: 32
  accumulate_grad_batches: 2
  num_epochs: 50
  gradient_clip_val: 1.0
  
  # Mixed precision
  precision: "bf16-mixed"  # Options: 32, 16-mixed, bf16-mixed
  
  # Regularization
  dropout: 0.1
  weight_decay: 0.01
  
  # Checkpointing
  save_every_n_steps: 5000
  validate_every_n_steps: 1000

# Fine-tuning Configuration
fine_tuning:
  # Which components to fine-tune
  tune_backbone: false
  tune_action_head: true
  tune_vision_encoder: false
  
  # LoRA settings for backbone
  lora:
    enabled: true
    rank: 64
    alpha: 128
    dropout: 0.1
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    
  # Learning rates for different components
  component_lrs:
    backbone: 1.0e-5
    action_head: 1.0e-4
    vision_encoder: 1.0e-5

# Data Configuration
data:
  # Data keys
  image_keys: ["observation.images.front", "observation.images.left", "observation.images.right", "observation.images.wrist"]
  state_keys: ["observation.state", "observation.joint_positions", "observation.joint_velocities"]
  action_keys: ["action"]
  
  # Temporal configuration
  fps: 30  # Frames per second
  delta_timestamps:
    action: [0.0, 0.033, 0.066, 0.1, 0.133, 0.166, 0.2, 0.233, 0.266, 0.3, 0.333, 0.366, 0.4, 0.433, 0.466, 0.5]
    observation.images.front: [0.0]
    observation.images.left: [0.0]
    observation.images.right: [0.0]
    observation.images.wrist: [0.0]
    observation.state: [0.0]
    observation.joint_positions: [0.0]
    observation.joint_velocities: [0.0]
    
  # Data augmentation
  augmentation:
    enabled: true
    image_aug:
      random_crop: true
      color_jitter: true
      random_rotation: 5  # degrees
      gaussian_noise: 0.01
    
# Evaluation Configuration
evaluation:
  n_episodes: 100
  max_episode_steps: 1000
  
  # Task-specific metrics
  metrics:
    - "success_rate"
    - "episode_length"
    - "task_completion_time"
    - "action_smoothness"
    - "collision_rate"

# Hardware Configuration
device:
  type: "auto"  # Options: auto, cpu, cuda, mps
  compile: false
  
  # Memory optimization
  gradient_checkpointing: true
  cpu_offload: false
  
  # Multi-GPU settings
  use_ddp: false
  num_gpus: 1

# Model Checkpointing
checkpoint:
  save_dir: "./checkpoints/gr00t_n1"
  save_top_k: 5
  monitor: "val_success_rate"
  mode: "max"
  
  # Model format
  save_format: "safetensors"

# Logging Configuration
logging:
  wandb:
    enabled: false
    project: "gr00t_n1"
    entity: null
    
  tensorboard:
    enabled: true
    log_dir: "./logs/gr00t_n1"
    
  # Video logging
  video_logging:
    enabled: true
    log_frequency: 10  # episodes
    max_videos_per_log: 5

# Environment-specific presets
presets:
  humanoid_locomotion:
    embodiment:
      robot_type: "humanoid"
    sequence:
      action_horizon: 16
      action_chunking: 4
    training:
      batch_size: 16
      
  manipulation:
    embodiment:
      robot_type: "manipulator"
    sequence:
      action_horizon: 8
      action_chunking: 2
    vision:
      multi_view:
        num_views: 2  # front and wrist only
        
  mobile_manipulation:
    embodiment:
      robot_type: "mobile_manipulator"
    sequence:
      action_horizon: 12
      action_chunking: 3
    vision:
      multi_view:
        num_views: 4
