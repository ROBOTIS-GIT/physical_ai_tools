# Docker Manager for Physical AI Tools

Physical AI Toolsì˜ Docker ì»¨í…Œì´ë„ˆ ìƒëª…ì£¼ê¸°ë¥¼ ê´€ë¦¬í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤. LeRobot, GR00T N1.5, Pi0 ë“± ë‹¤ì–‘í•œ Physical AI í”„ë ˆì„ì›Œí¬ë¥¼ Docker ì»¨í…Œì´ë„ˆë¡œ ì‹¤í–‰í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.

## ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥

### 1. Image Management
- Docker ì´ë¯¸ì§€ ë¹Œë“œ
- ì´ë¯¸ì§€ Pull/Push
- ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ ë° ì‚­ì œ

### 2. Container Lifecycle
- ì»¨í…Œì´ë„ˆ ìƒì„±/ì‹œì‘/ì¤‘ì§€/ì¬ì‹œì‘/ì‚­ì œ
- ì»¨í…Œì´ë„ˆ ìƒíƒœ ëª¨ë‹ˆí„°ë§
- ì—¬ëŸ¬ í”„ë ˆì„ì›Œí¬ ë™ì‹œ ì‹¤í–‰

### 3. Process Management
- ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ëª…ë ¹ ì‹¤í–‰
- í”„ë ˆì„ì›Œí¬ë³„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘/ê´€ë¦¬
- í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§

### 4. Network & Communication
- ì»¨í…Œì´ë„ˆ ê°„ ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
- HTTP/gRPC/ZMQ í†µì‹  ì„¤ì •
- IP ì£¼ì†Œ ë° í¬íŠ¸ ê´€ë¦¬

### 5. Resource Management
- GPU í• ë‹¹ ë° ê´€ë¦¬
- ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
- ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡°
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Physical AI Server                      â”‚
â”‚              (ROS2 Node - Orchestrator)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            Docker Manager                          â”‚ â”‚
â”‚  â”‚  - Image Management                                â”‚ â”‚
â”‚  â”‚  - Container Lifecycle                             â”‚ â”‚
â”‚  â”‚  - Resource Allocation                             â”‚ â”‚
â”‚  â”‚  - Communication Setup                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚               â”‚                â”‚
         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
         â”‚  LeRobot   â”‚  â”‚  GR00T   â”‚   â”‚    Pi0     â”‚
         â”‚ Container  â”‚  â”‚Container â”‚   â”‚ Container  â”‚
         â”‚            â”‚  â”‚          â”‚   â”‚            â”‚
         â”‚ API Server â”‚  â”‚API Serverâ”‚   â”‚ API Server â”‚
         â”‚  Port:8000 â”‚  â”‚Port:8001 â”‚   â”‚ Port:8002  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚               â”‚                â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    Docker Network Bridge
                  (physical_ai_network)
```

### í†µì‹  ì „ëµ

#### âŒ ë¬¸ì œ: ROS2 DDS ë²„ì „ ë¶ˆì¼ì¹˜
```
LeRobot Container (Ubuntu 22.04 + ROS2 Humble)
     â†•ï¸ DDS í†µì‹  ë¶ˆê°€ëŠ¥ (ë²„ì „ ë¶ˆì¼ì¹˜)
GR00T Container (Ubuntu 20.04 + ROS2 Foxy)
```

#### âœ… í•´ê²°: ZMQ ê¸°ë°˜ í†µí•© í†µì‹ 
```
[Robot/External Device]
         â†• ROS2 DDS (Same version)
[Physical AI Server]
         â†• ZMQ (Version independent, High performance)
[Framework Containers]
  - LeRobot Container (ZMQ Server)
  - GR00T Container (ZMQ Server)
  - Pi0 Container (ZMQ Server)
         â†• Shared Docker Volumes (For large data: datasets, models)
[Host File System]
```

**ZMQ í†µì‹  ì•„í‚¤í…ì²˜:**
```
Physical AI Server
â”œâ”€â”€ ZMQ Client Manager
â”‚   â”œâ”€â”€ LeRobot Client (tcp://lerobot_container:5555)
â”‚   â”œâ”€â”€ GR00T Client (tcp://groot_container:5556)
â”‚   â””â”€â”€ Pi0 Client (tcp://pi0_container:5557)
â””â”€â”€ ROS2 Interface (ì™¸ë¶€ ë¡œë´‡ê³¼ í†µì‹ )

ê° Framework Container
â”œâ”€â”€ ZMQ Server (REQ-REP or PUB-SUB pattern)
â”œâ”€â”€ Framework Core (LeRobot/GR00T/Pi0)
â””â”€â”€ Shared Volume Access
```

### ì™œ ZMQë¥¼ ì„ íƒí–ˆëŠ”ê°€?

| ì´ìœ  | ì„¤ëª… |
|------|------|
| **ğŸš€ ê³ ì„±ëŠ¥** | ë§¤ìš° ë¹ ë¥¸ ë©”ì‹œì§€ ì „ì†¡ (ì¶”ë¡ ì— ì¤‘ìš”) |
| **ğŸ”„ ì¼ê´€ì„±** | ì´ë¯¸ Inferenceì—ì„œ ZMQ ì‚¬ìš© ì¤‘ |
| **ğŸŒ ë²„ì „ ë…ë¦½** | ROS2 ë²„ì „ê³¼ ë¬´ê´€í•˜ê²Œ ë™ì‘ |
| **ğŸ¯ ê°„ë‹¨í•¨** | HTTPë³´ë‹¤ ê°€ë³ê³ , gRPCë³´ë‹¤ ì„¤ì • ê°„ë‹¨ |
| **ğŸ“¦ ë‹¤ì–‘í•œ íŒ¨í„´** | REQ-REP, PUB-SUB, PUSH-PULL ë“± |
| **ğŸ Python ì¹œí™”ì ** | pyzmqë¡œ ì‰¬ìš´ í†µí•© |

### ZMQ vs ë‹¤ë¥¸ í”„ë¡œí† ì½œ

| í”„ë¡œí† ì½œ | ì‚¬ìš©ì²˜ | ì¥ì  | ë‹¨ì  |
|---------|------|------|------|
| **ZMQ** | **ëª¨ë“  ì»¨í…Œì´ë„ˆ í†µì‹ ** âœ… | ë§¤ìš° ë¹ ë¦„, ë¹„ë™ê¸°, ë‹¤ì–‘í•œ íŒ¨í„´ | ë©”ì‹œì§€ í˜•ì‹ ì§ì ‘ ì •ì˜ |
| **Shared Volume** | ëŒ€ìš©ëŸ‰ ë°ì´í„° (ë°ì´í„°ì…‹, ëª¨ë¸) | ë¹ ë¥¸ I/O, íŒŒì¼ ê³µìœ  | ë™ê¸°í™” í•„ìš” |
| **ROS2 DDS** | Physical AI Server â†” ë¡œë´‡ | Native ROS2, í‘œì¤€ | ë²„ì „ ì˜ì¡´ì„± |
| ~~HTTP REST~~ | âŒ ì‚¬ìš© ì•ˆí•¨ | ê°„ë‹¨, ë””ë²„ê¹… ìš©ì´ | ëŠë¦¼ |
| ~~gRPC~~ | âŒ ì‚¬ìš© ì•ˆí•¨ | íƒ€ì… ì•ˆì •ì„± | ë³µì¡í•œ ì„¤ì • |

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì„¤ì¹˜

```bash
# Docker SDKì™€ ZMQ ì„¤ì¹˜
pip install docker>=7.0.0 pyzmq>=25.0.0

# ë˜ëŠ” ROS2 íŒ¨í‚¤ì§€ ë¹Œë“œ ì‹œ ìë™ ì„¤ì¹˜
cd ~/ros2_ws
colcon build --packages-select physical_ai_server
```

### ê¸°ë³¸ ì‚¬ìš©ë²•

#### Physical AI Server ì¸¡ (ZMQ Client)

```python
from physical_ai_server.docker_manager import (
    DockerManager,
    FrameworkType,
    ZMQClientPool,
)

# Docker Manager ì´ˆê¸°í™”
manager = DockerManager()

# LeRobot ì»¨í…Œì´ë„ˆ ìƒì„± ë° ì‹œì‘
container_id = manager.create_container(
    framework=FrameworkType.LEROBOT,
    gpu_ids=[0],
    memory_limit='8g',
    api_port=5555,  # ZMQ í¬íŠ¸
)
manager.start_container(container_id)

# ZMQ ì„œë²„ ì‹œì‘ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€)
process_config = {
    'mode': 'inference',
    'port': 5555,
}
manager.start_framework_process(container_id, FrameworkType.LEROBOT, process_config)

# ZMQ í´ë¼ì´ì–¸íŠ¸ í’€ ìƒì„±
client_pool = ZMQClientPool()

# ì»¨í…Œì´ë„ˆ IP ê°€ì ¸ì˜¤ê¸° ë° í´ë¼ì´ì–¸íŠ¸ ì¶”ê°€
container_ip = manager.get_container_ip(container_id)
client_pool.add_client('lerobot', f'tcp://{container_ip}:5555')

# Inference ìš”ì²­
observation = {
    'image': [...],  # ì´ë¯¸ì§€ ë°ì´í„°
    'robot_state': [...],  # ë¡œë´‡ ìƒíƒœ
}
response = client_pool.inference('lerobot', observation)
print(f"Action: {response['result']['action']}")
```

#### Framework Container ì¸¡ (ZMQ Server)

```python
# LeRobot ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ì‹¤í–‰
from physical_ai_server.docker_manager import ZMQServer, MessageType

# Inference í•¸ë“¤ëŸ¬
def handle_inference(data):
    observation = data['observation']
    # LeRobotìœ¼ë¡œ ì¶”ë¡  ì‹¤í–‰
    action = model.predict(observation)
    return {'action': action}

# ZMQ ì„œë²„ ì‹œì‘
server = ZMQServer('tcp://*:5555')
server.register_handler(MessageType.INFERENCE, handle_inference)
server.run()  # ë¸”ë¡œí‚¹ - ê³„ì† ì‹¤í–‰ë¨
```

## ğŸ“š ìƒì„¸ ì‚¬ìš© ì˜ˆì‹œ

### 1. ì—¬ëŸ¬ í”„ë ˆì„ì›Œí¬ ë™ì‹œ ì‹¤í–‰

```python
from physical_ai_server.docker_manager import (
    DockerManager,
    FrameworkType,
    CommunicationProtocol,
)

manager = DockerManager()

# ì—¬ëŸ¬ í”„ë ˆì„ì›Œí¬ ì»¨í…Œì´ë„ˆ ìƒì„±
frameworks = [
    (FrameworkType.LEROBOT, 8000),
    (FrameworkType.GROOT_N15, 8001),
    (FrameworkType.PI0, 8002),
]

containers = {}
for framework, port in frameworks:
    container_id = manager.create_container(
        framework=framework,
        api_port=port,
        gpu_ids=None,  # ëª¨ë“  GPU ê³µìœ 
    )
    manager.start_container(container_id)
    containers[framework] = container_id

# ZMQ í†µì‹  ì„¤ì • (ëª¨ë“  ì»¨í…Œì´ë„ˆëŠ” ZMQ Serverë¡œ ë™ì‘)
comm_config = manager.setup_communication(
    source_framework=FrameworkType.LEROBOT,
    target_framework=FrameworkType.GROOT_N15,
    protocol=CommunicationProtocol.ZMQ,
)
print(f"ZMQ Endpoint: {comm_config['endpoint']}")  # tcp://groot_ip:8001
```

### 2. í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬

```python
# LeRobot ì¶”ë¡  ì„œë²„ ì‹œì‘
process_config = {
    'mode': 'inference',
    'model_path': '/workspace/shared_data/models/lerobot_policy',
    'port': 8000,
}

process_info = manager.start_framework_process(
    container_id=container_id,
    framework=FrameworkType.LEROBOT,
    config=process_config,
)

print(f"Process PID: {process_info['pid']}")
print(f"Log file: {process_info['log_file']}")

# í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§
status = manager.monitor_process(container_id, process_info['pid'])
print(f"CPU: {status['cpu_percent']}%")
print(f"Elapsed: {status['elapsed_time']}")
```

### 3. ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ëª…ë ¹ ì‹¤í–‰

```python
# ë‹¨ì¼ ëª…ë ¹ ì‹¤í–‰
output = manager.exec_command(
    container_id,
    "python -c 'import torch; print(torch.cuda.is_available())'",
)
print(f"CUDA Available: {output}")

# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
for line in manager.exec_command(
    container_id,
    "python train.py",
    stream=True,
):
    print(line.decode('utf-8'), end='')
```

### 4. ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

```python
# ë©”ëª¨ë¦¬ ì œí•œ ì—…ë°ì´íŠ¸
manager.set_memory_limits(
    container_id,
    memory_limit='16g',  # 16GBë¡œ ì¦ê°€
)

# ì‹¤ì‹œê°„ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
import time
for i in range(10):
    resources = manager.monitor_resources(container_id)
    print(f"[{i}] CPU: {resources['cpu_percent']:5.1f}% | "
          f"Memory: {resources['memory_percent']:5.1f}% | "
          f"Network RX: {resources['network_rx_bytes']/(1024**2):.2f}MB")
    time.sleep(1)
```

## ğŸ”§ Configuration

### í™˜ê²½ ë³€ìˆ˜

```bash
# Docker ë°ëª¬ ì ‘ì† (ê¸°ë³¸: unix:///var/run/docker.sock)
export DOCKER_HOST=unix:///var/run/docker.sock

# GPU ì‚¬ìš© ì„¤ì •
export NVIDIA_VISIBLE_DEVICES=0,1  # GPU 0, 1 ì‚¬ìš©
```

### Docker ë„¤íŠ¸ì›Œí¬ ì„¤ì •

```python
manager = DockerManager(
    network_name="custom_network",
    shared_volume_path="/custom/path",
    base_image_registry="myregistry",
)
```

### ë³¼ë¥¨ ë§¤í•‘

Docker ManagerëŠ” ìë™ìœ¼ë¡œ ë‹¤ìŒ ë³¼ë¥¨ì„ ìƒì„±í•©ë‹ˆë‹¤:

```
Host                                    â†’ Container
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
~/.cache/physical_ai_tools/shared_data  â†’ /workspace/shared_data
~/.cache/physical_ai_tools/lerobot      â†’ /workspace/lerobot_data
~/.cache/physical_ai_tools/groot_n15    â†’ /workspace/groot_n15_data
~/.cache/huggingface                    â†’ /root/.cache/huggingface
```

## ğŸ› ë¬¸ì œ í•´ê²°

### Docker ë°ëª¬ ì—°ê²° ì‹¤íŒ¨

```bash
# Docker ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl start docker

# ì‚¬ìš©ìë¥¼ docker ê·¸ë£¹ì— ì¶”ê°€
sudo usermod -aG docker $USER
newgrp docker
```

### NVIDIA Runtime ì˜¤ë¥˜

```bash
# NVIDIA Container Toolkit ì„¤ì¹˜
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
    sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### ì»¨í…Œì´ë„ˆ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ

```bash
# ë„¤íŠ¸ì›Œí¬ ì¬ìƒì„±
docker network rm physical_ai_network
docker network create physical_ai_network

# ë„¤íŠ¸ì›Œí¬ í™•ì¸
docker network inspect physical_ai_network
```

### í¬íŠ¸ ì¶©ëŒ

```python
# ì‚¬ìš© ê°€ëŠ¥í•œ í¬íŠ¸ ìë™ í• ë‹¹
import socket

def find_free_port():
    with socket.socket() as s:
        s.bind(('', 0))
        return s.getsockname()[1]

port = find_free_port()
container_id = manager.create_container(
    framework=FrameworkType.LEROBOT,
    api_port=port,
)
```

## ğŸ“– API Reference

### DockerManager í´ë˜ìŠ¤

#### ì´ˆê¸°í™”

```python
DockerManager(
    network_name: str = "physical_ai_network",
    shared_volume_path: Optional[Path] = None,
    base_image_registry: str = "robotis",
    logger: Optional[logging.Logger] = None,
)
```

#### Image Management

- `build_image(framework, dockerfile_path, platform=None, build_args=None, tag=None)` â†’ Image
- `pull_image(framework, tag="latest")` â†’ Image
- `list_images(framework=None)` â†’ List[Dict]
- `remove_image(framework, tag="latest", force=False)` â†’ None

#### Container Lifecycle

- `create_container(framework, config=None, gpu_ids=None, memory_limit=None, api_port=None)` â†’ str
- `start_container(container_id)` â†’ None
- `stop_container(container_id, timeout=10)` â†’ None
- `restart_container(container_id, timeout=10)` â†’ None
- `remove_container(container_id, force=False, volumes=False)` â†’ None
- `get_container_status(container_id)` â†’ ContainerStatus
- `list_containers(all_containers=True)` â†’ List[Dict]

#### Process Management

- `exec_command(container_id, command, workdir=None, environment=None, stream=False)` â†’ str
- `start_framework_process(container_id, framework, config)` â†’ Dict
- `monitor_process(container_id, process_id)` â†’ Dict

#### Network & Communication

- `connect_containers(container_ids, network=None)` â†’ None
- `get_container_ip(container_id)` â†’ Optional[str]
- `setup_communication(source_framework, target_framework, protocol)` â†’ Dict

#### Resource Management

- `allocate_gpu(container_id, gpu_ids)` â†’ None (Not implemented - requires recreation)
- `set_memory_limits(container_id, memory_limit, memory_swap=None)` â†’ None
- `monitor_resources(container_id)` â†’ Dict

### Enums

#### FrameworkType
- `LEROBOT`: LeRobot í”„ë ˆì„ì›Œí¬
- `GROOT_N15`: GR00T N1.5 í”„ë ˆì„ì›Œí¬
- `PI0`: Pi0 í”„ë ˆì„ì›Œí¬
- `ISAAC_SIM`: Isaac Sim
- `ISAAC_LAB`: Isaac Lab

#### ContainerStatus
- `NOT_EXIST`, `CREATED`, `RUNNING`, `PAUSED`, `RESTARTING`, `REMOVING`, `EXITED`, `DEAD`

#### CommunicationProtocol
- `ZMQ`: ZeroMQ (ê¸°ë³¸ í†µì‹  í”„ë¡œí† ì½œ) âœ…
- `SHARED_VOLUME`: ê³µìœ  ë³¼ë¥¨ (ëŒ€ìš©ëŸ‰ ë°ì´í„°ìš©)

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# ZMQ í†µì‹  ì˜ˆì œ ì‹¤í–‰
cd ~/ros2_ws/src/physical_ai_tools/physical_ai_server
python -m physical_ai_server.docker_manager.zmq_examples

# Docker Manager ì˜ˆì œ ì‹¤í–‰
python -m physical_ai_server.docker_manager.examples
```

### ZMQ í†µì‹  í…ŒìŠ¤íŠ¸

#### 1. í„°ë¯¸ë„ 1: ZMQ ì„œë²„ ì‹œì‘
```bash
python -c "
from physical_ai_server.docker_manager import ZMQServer, MessageType
import time

def handle_inference(data):
    print(f'Received: {data}')
    return {'action': [0.1, 0.2, 0.3]}

server = ZMQServer('tcp://*:5555')
server.register_handler(MessageType.INFERENCE, handle_inference)
server.run()
"
```

#### 2. í„°ë¯¸ë„ 2: ZMQ í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸
```bash
python -c "
from physical_ai_server.docker_manager import ZMQClient

client = ZMQClient('tcp://localhost:5555')
response = client.inference({'image': [1,2,3]})
print(f'Response: {response}')
client.close()
"
```

## ğŸ”„ Physical AI Server í†µí•©

```python
# physical_ai_server.pyì—ì„œ ì‚¬ìš©

from physical_ai_server.docker_manager import (
    DockerManager,
    FrameworkType,
    ZMQClientPool,
)

class PhysicalAIServer(Node):
    def __init__(self):
        super().__init__('physical_ai_server')
        
        # Docker Manager ì´ˆê¸°í™”
        self.docker_manager = DockerManager(
            logger=self.get_logger(),
        )
        
        # ZMQ Client Pool ì´ˆê¸°í™”
        self.zmq_clients = ZMQClientPool(
            logger=self.get_logger(),
        )
        
        # í™œì„± ì»¨í…Œì´ë„ˆ ê´€ë¦¬
        self.active_containers = {}
    
    def start_framework(self, framework_name: str):
        """ì‚¬ìš©ì ìš”ì²­ ì‹œ í”„ë ˆì„ì›Œí¬ ì»¨í…Œì´ë„ˆ ì‹œì‘"""
        framework = FrameworkType(framework_name)
        
        # 1. ì»¨í…Œì´ë„ˆ ìƒì„± ë° ì‹œì‘
        container_id = self.docker_manager.create_container(
            framework=framework,
            gpu_ids=[0],
            api_port=5555,  # ZMQ í¬íŠ¸
        )
        self.docker_manager.start_container(container_id)
        
        # 2. ZMQ ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
        config = {
            'mode': 'inference',
            'port': 5555,
        }
        process_info = self.docker_manager.start_framework_process(
            container_id, framework, config
        )
        
        # 3. ZMQ í´ë¼ì´ì–¸íŠ¸ ì¶”ê°€
        container_ip = self.docker_manager.get_container_ip(container_id)
        endpoint = f'tcp://{container_ip}:5555'
        self.zmq_clients.add_client(framework_name, endpoint)
        
        # 4. ìƒíƒœ ì €ì¥
        self.active_containers[framework_name] = {
            'container_id': container_id,
            'process_info': process_info,
            'endpoint': endpoint,
        }
        
        self.get_logger().info(f"Started {framework_name}: {endpoint}")
    
    def run_inference(self, framework_name: str, observation: dict):
        """ì¶”ë¡  ì‹¤í–‰"""
        response = self.zmq_clients.inference(framework_name, observation)
        
        if response and response.get('status') == 'success':
            return response['result']['action']
        else:
            self.get_logger().error(f"Inference failed: {response}")
            return None
    
    def stop_framework(self, framework_name: str):
        """í”„ë ˆì„ì›Œí¬ ì»¨í…Œì´ë„ˆ ì¤‘ì§€"""
        if framework_name in self.active_containers:
            container_id = self.active_containers[framework_name]['container_id']
            
            # ZMQ í´ë¼ì´ì–¸íŠ¸ ì œê±°
            self.zmq_clients.remove_client(framework_name)
            
            # ì»¨í…Œì´ë„ˆ ì¤‘ì§€ ë° ì œê±°
            self.docker_manager.stop_container(container_id)
            self.docker_manager.remove_container(container_id)
            
            del self.active_containers[framework_name]
            self.get_logger().info(f"Stopped {framework_name}")
```

## ğŸ“ License

Apache 2.0

## ğŸ‘¥ Authors

- Dongyun Kim (kdy@robotis.com)
- Physical AI Tools Team

## ğŸ¤ Contributing

Issuesì™€ Pull Requestsë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!

## ğŸ“ Support

ë¬¸ì œê°€ ìˆìœ¼ì‹œë©´ GitHub Issuesì— ë“±ë¡í•´ì£¼ì„¸ìš”.
